{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d1429f3-1876-4650-a5b9-d031d893ef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "페이지 제목: NAVER\n",
      "검색 결과 페이지 제목: 파이썬 : 네이버 검색\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "#options.add_argument('--headless')  # 브라우저가 보이지 않게 실행\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "time.sleep(5)\n",
    "\n",
    "driver.get('https://www.naver.com')\n",
    "print('페이지 제목:',driver.title)\n",
    "\n",
    "search_box = driver.find_element(By.NAME, 'query')\n",
    "search_box.send_keys('파이썬')\n",
    "search_box.submit()\n",
    "\n",
    "print(\"검색 결과 페이지 제목:\",driver.title)\n",
    "\n",
    "time.sleep(30)\n",
    "driver.quit()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb8df89-50b6-4ebd-8d1f-8902f91c7d4f",
   "metadata": {},
   "source": [
    "### 스크래핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86951a87-49d5-41ae-b466-0dd88dba7266",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"불법수사·압색\"… 뉴스타파, 국가에 손배 6억 청구',\n",
       " '파이낸셜뉴스 노조원 80% 회사 떠날 생각... 저임금 이유 93.8%',\n",
       " '언론사 여성 임원 감소... 중간 간부급 기자는 늘어',\n",
       " '대구시, 뉴스민에 100만원 손해배상 불복 항소',\n",
       " '폭설 끝나니 한파 ‘최저 -8도’…오후부터 다시 눈·비',\n",
       " '[단독]이재명 “여당이 시간 끌면 당론 입법 강행처리하라”',\n",
       " '10월 생산·소비·투자 모두 줄었다…‘트리플 감소’',\n",
       " '[여랑야랑]한동훈 다음 황태자? / 박형준, 여의도 천막극장? / 윤 대통령과 골프 ‘로또’?',\n",
       " \"[언론장악 카르텔]⑭ 尹 정권 '언론장악 돌격대'로 환생한 여론공작 '좀비들'\",\n",
       " \"[주간 뉴스타파] 꼬리에 꼬리를 무는 명태균 게이트 '완벽 정리'\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')  \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    def create_driver(self):\n",
    "        return webdriver.Chrome(options =self.options)\n",
    "\n",
    "    def scape_naver_headline(self):\n",
    "        driver = self.create_driver()\n",
    "        try:\n",
    "            driver.get('https://news.naver.com')\n",
    "            items = driver.find_elements(By.CSS_SELECTOR,'a._editn_link')[:10]\n",
    "            return[item.text.strip() for item in items]\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "scp = WebScraper()\n",
    "dt1 = scp.scape_naver_headline()\n",
    "dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7f7abdc3-21f8-4a36-8b5c-fa00b7a0cee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'A Light in the Attic', 'price': '£51.77', 'rate': 3}\n",
      "\n",
      "{'title': 'Tipping the Velvet', 'price': '£53.74', 'rate': 1}\n",
      "\n",
      "{'title': 'Soumission', 'price': '£50.10', 'rate': 1}\n",
      "\n",
      "{'title': 'Sharp Objects', 'price': '£47.82', 'rate': 4}\n",
      "\n",
      "{'title': 'Sapiens: A Brief History of Humankind', 'price': '£54.23', 'rate': 5}\n",
      "\n",
      "{'title': 'The Requiem Red', 'price': '£22.65', 'rate': 1}\n",
      "\n",
      "{'title': 'The Dirty Little Secrets of Getting Your Dream Job', 'price': '£33.34', 'rate': 4}\n",
      "\n",
      "{'title': 'The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull', 'price': '£17.93', 'rate': 3}\n",
      "\n",
      "{'title': 'The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics', 'price': '£22.60', 'rate': 4}\n",
      "\n",
      "{'title': 'The Black Maria', 'price': '£52.15', 'rate': 1}\n",
      "\n",
      "{'title': 'Starving Hearts (Triangular Trade Trilogy, #1)', 'price': '£13.99', 'rate': 2}\n",
      "\n",
      "{'title': \"Shakespeare's Sonnets\", 'price': '£20.66', 'rate': 4}\n",
      "\n",
      "{'title': 'Set Me Free', 'price': '£17.46', 'rate': 5}\n",
      "\n",
      "{'title': \"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\", 'price': '£52.29', 'rate': 5}\n",
      "\n",
      "{'title': 'Rip it Up and Start Again', 'price': '£35.02', 'rate': 5}\n",
      "\n",
      "{'title': 'Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991', 'price': '£57.25', 'rate': 3}\n",
      "\n",
      "{'title': 'Olio', 'price': '£23.88', 'rate': 1}\n",
      "\n",
      "{'title': 'Mesaerion: The Best Science Fiction Stories 1800-1849', 'price': '£37.59', 'rate': 1}\n",
      "\n",
      "{'title': 'Libertarianism for Beginners', 'price': '£51.33', 'rate': 2}\n",
      "\n",
      "{'title': \"It's Only the Himalayas\", 'price': '£45.17', 'rate': 2}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "def book():\n",
    "    url = \"https://books.toscrape.com/\"\n",
    "   \n",
    "    options = Options()\n",
    "    #options.add_argument('--headless')  \n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    service = Service('chromedriver.exe')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "     \n",
    "        time.sleep(5) \n",
    "       \n",
    "        posts = []\n",
    "        items = driver.find_elements(By.CSS_SELECTOR, 'article.product_pod')\n",
    "        \n",
    "        rt = {'One':1,'Two':2,'Three':3,'Four':4,'Five':5}\n",
    "        for item in items:\n",
    "            title = item.find_element(By.CSS_SELECTOR, 'h3 a').get_attribute('title')\n",
    "            price = item.find_element(By.CSS_SELECTOR, 'p.price_color').text.strip()\n",
    "            rate = rt[item.find_element(By.CSS_SELECTOR, 'p.star-rating').get_attribute('class').split()[-1]]\n",
    "    \n",
    "         \n",
    "            posts.append({\n",
    "                'title': title,\n",
    "                'price': price,\n",
    "                'rate':rate\n",
    "            })\n",
    "        return posts\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "book_data = book()\n",
    "for data in book_data:\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f236dcd4-a979-4f13-8140-e9825c11fcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'temperature': '현재 온도\\n0.3°', 'weather': '맑음'}]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "def weather():\n",
    "    url = \"https://weather.naver.com/\"\n",
    "   \n",
    "    options = Options()\n",
    "    #options.add_argument('--headless')  \n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    service = Service('chromedriver.exe')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "     \n",
    "        time.sleep(5) \n",
    "       \n",
    "        posts = []\n",
    "        items = driver.find_elements(By.CSS_SELECTOR, 'div.weather_area._cnBlockTemplate')\n",
    "        \n",
    "        for item in items:\n",
    "            temperature = item.find_element(By.CSS_SELECTOR,'strong.current').text.strip()\n",
    "            weather = item.find_element(By.CSS_SELECTOR, 'span.weather').text.strip()\n",
    "    \n",
    "         \n",
    "            posts.append({\n",
    "                'temperature': temperature,\n",
    "                'weather': weather,\n",
    "            })\n",
    "        return posts\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ffb6a7fb-498b-41cc-839b-9b95bbb525e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (774353688.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[138], line 26\u001b[1;36m\u001b[0m\n\u001b[1;33m    return {'repo':repo_name,'stars':stars,'forks':forks,'repo_description':repo_description}\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')  \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    def create_driver(self):\n",
    "        return webdriver.Chrome(options =self.options)\n",
    "\n",
    "    def scrape_github_repo(self, username, repo):\n",
    "        driver = self.create_driver()\n",
    "        try:\n",
    "            driver.get(f'http://github/{username}/{repo}')\n",
    "        \n",
    "            repo_description = driver.find_element(By.CSS_SELECTOR,'p.my3').text.strip()\n",
    "            repo_name = driver.find_element(By.CSS_SELECTOR,'strong.mr-2 a').text.strip()\n",
    "            stars = driver.find_element(By.CSS_SELECTOR, '#repo-stars-counter-star').text.strip()\n",
    "            forks = driver.find_element(By.CSS_SELECTOR, '#repo-network-counter').text.strip()\n",
    "        return {'repo':repo_name,'stars':stars,'forks':forks,'repo_description':repo_description}\n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "scp = WebScraper()\n",
    "d4 = scp.scrape_github_repo('gilbutItbook','080235')\n",
    "d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8438aee-77bc-43f0-a60a-cab0a1e53bbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': '삼성전자', 'code': '005930', 'price': '54200'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless')  \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    def create_driver(self):\n",
    "        return webdriver.Chrome(options =self.options)\n",
    "\n",
    "    def scape_naver_headline(self):\n",
    "        driver = self.create_driver()\n",
    "        try:\n",
    "            driver.get('https://finance.naver.com/item/main.naver?code=005930')\n",
    "            name = driver.find_element(By.CSS_SELECTOR,'.wrap_company a').text.strip()\n",
    "            code = driver.find_element(By.CSS_SELECTOR,'span.code').text.strip()\n",
    "            price = driver.find_element(By.CSS_SELECTOR, 'p.no_today').text.replace('\\n','').replace(',','')\n",
    "           \n",
    "            return {'name':name,'code':code,'price':price}\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "scp = WebScraper()\n",
    "dt1 = scp.scape_naver_headline()\n",
    "dt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d905fb62-b2c6-4488-a1e5-8950b4f7201b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오류\n",
    "#price =int('',.join([k.text for k in driver.find_element(By.CSS_SELECTOR, '.no_today em span')]).replace(',',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dae2cc-3799-4cf1-86aa-bdcdcd8448ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        #self.options.add_argument('--headless')  \n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    def create_driver(self):\n",
    "        return webdriver.Chrome(options =self.options)\n",
    "\n",
    "    def scape_naver_headline(self):\n",
    "        driver = self.create_driver()\n",
    "        try:\n",
    "            driver.get('https://m.stock.naver.com/domestic/stock/005930/total')\n",
    "            priceAll = WebDriverWait(driver, 60).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'strong.GrapMain_price__GT8dV'))\n",
    "            )\n",
    "            price = int(priceAll.text.replace('\\n','').replace(',','').replace('원','').strip())\n",
    "            name = driver.find_element(By.CSS_SELECTOR,'.GrapMain_name__r5bQX').text.strip()\n",
    "            code = driver.find_element(By.CSS_SELECTOR,'.GrapMain_code__1MqvF').text.strip()\n",
    "            \n",
    "           \n",
    "            return {'name':name,'code':code,'price':price}\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "scp = WebScraper()\n",
    "dt2 = scp.scape_naver_headline()\n",
    "dt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ec9762e3-0edd-4160-a240-1934a78a679d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 완료\n",
      "2페이지 완료\n",
      "3페이지 완료\n",
      "\n",
      "=== 검색 결과 ===\n",
      "검색어:인공지능\n",
      "수집 시간: 2024-11-29 16:16:21\n",
      "총 30개의 기사 수집\n",
      "\n",
      "1. AI교과서 첫 검정 심사 통과율 52%…21곳 중 12곳만 합격\n",
      "    링크 : https://www.newsis.com/view/NISX20241128_0002977168\n",
      "\n",
      "2. 산·학·연 함께하는 인공지능 네트워크 포럼 출범\n",
      "    링크 : https://www.yna.co.kr/view/AKR20241129031400063?input=1195m\n",
      "\n",
      "3. 울주군, 인공지능 스마트 도로방범 시스템 전국 첫 구축\n",
      "    링크 : https://www.seoul.co.kr/news/society/2024/11/29/20241129500102?wlog_tag3=naver\n",
      "\n",
      "4. \"인공지능이 불러올 위험에 맞선다\"... 'AI 안전연구소' 출범\n",
      "    링크 : http://www.fnnews.com/news/202411270945482485\n",
      "\n",
      "5. KT ds, 인공지능 활용 사내 업무 툴 '웍스 AI' 오픈…MS 애저 기반\n",
      "    링크 : https://view.asiae.co.kr/article/2024112909091743125\n",
      "\n",
      "6. 인공지능 디지털교과서 76종 검정 합격\n",
      "    링크 : https://www.naeil.com/news/read/530587?ref=naver\n",
      "\n",
      "7. ‘AI와 인간중심주의’ 인공지능총서 100번째 타이틀\n",
      "    링크 : https://www.kmib.co.kr/article/view.asp?arcid=1732697177&code=13150000&cp=nv\n",
      "\n",
      "8. 현대백화점, 겨울학기에 '인공지능' 강좌 첫 개설\n",
      "    링크 : https://www.goodkyung.com/news/articleView.html?idxno=252281\n",
      "\n",
      "9. 국표원, 인공지능 경영시스템 국가표준 제정 예고\n",
      "    링크 : https://www.newsis.com/view/NISX20241127_0002974186\n",
      "\n",
      "10. \"광주·이스라엘 문화 매개로 인공지능 분야 교류 확대 약속\"\n",
      "    링크 : https://www.newsis.com/view/NISX20241129_0002978185\n",
      "\n",
      "11. KAIST, ‘바이오 경로’ 이미지 분석 인공지능 개발\n",
      "    링크 : https://view.asiae.co.kr/article/2024112808555493952\n",
      "\n",
      "12. 인공지능 업계 종사자, 연구자 모여 'AI 윤리 쟁점' 논의\n",
      "    링크 : https://www.news1.kr/it-science/general-it/5614139\n",
      "\n",
      "13. 광주시 28~29일 인공지능 전시회 'AI TECH+'\n",
      "    링크 : https://www.news1.kr/local/gwangju-jeonnam/5613505\n",
      "\n",
      "14. 카카오, 유엔 회의서 'AI-이용자 건강한 상호작용 윤리' 발표\n",
      "    링크 : https://www.newsis.com/view/NISX20241126_0002972533\n",
      "\n",
      "15. 광주시, 광주형 인공지능(AI) 비즈니스 245~252번째 업무협약\n",
      "    링크 : http://www.enewstoday.co.kr/news/articleView.html?idxno=2206546\n",
      "\n",
      "16. 인공지능 디지털교과서 76종 검정 합격\n",
      "    링크 : https://www.naeil.com/news/read/530587?ref=naver\n",
      "\n",
      "17. ‘AI와 인간중심주의’ 인공지능총서 100번째 타이틀\n",
      "    링크 : https://www.kmib.co.kr/article/view.asp?arcid=1732697177&code=13150000&cp=nv\n",
      "\n",
      "18. 현대백화점, 겨울학기에 '인공지능' 강좌 첫 개설\n",
      "    링크 : https://www.goodkyung.com/news/articleView.html?idxno=252281\n",
      "\n",
      "19. 딥엘, 실시간 음성 번역 솔루션 공개…\"언어장벽 따른 업무 장애 해소\"\n",
      "    링크 : https://www.newsis.com/view/NISX20241128_0002976120\n",
      "\n",
      "20. 해수부, 항만 보안에 인공지능·안티드론까지 총 동원한다\n",
      "    링크 : http://news.mt.co.kr/mtview.php?no=2024112817083526681\n",
      "\n",
      "21. 인공지능은 의식을 가질 수 있을까[이경전의 행복한 AI 읽기](16)\n",
      "    링크 : https://weekly.khan.co.kr/khnm.html?mode=view&code=114&artid=202411291550041\n",
      "\n",
      "22. LG유플러스, 인공지능 중심 조직개편\n",
      "    링크 : https://www.naeil.com/news/read/530473?ref=naver\n",
      "\n",
      "23. 빅밸류, 하나은행에 인공지능 시세 조회 서비스 제공\n",
      "    링크 : http://www.edaily.co.kr/news/newspath.asp?newsid=02299286639090312\n",
      "\n",
      "24. \"80개국 언어 구사\"…날씨 전하던 日 아나운서의 실체\n",
      "    링크 : https://view.asiae.co.kr/article/2024112909205012684\n",
      "\n",
      "25. 삼성전자 2025년 임원 인사, \"인공지능, 반도체 다수 승진\"...'부사장 35명, 상무 92명, Master 10명' 총 137명 승진\n",
      "    링크 : https://www.aitimes.kr/news/articleView.html?idxno=33028\n",
      "\n",
      "26. 영화의전당, '부산국제인공지능영화제' AI 영화 상영 나서\n",
      "    링크 : https://www.nocutnews.co.kr/news/6251364?utm_source=naver&utm_medium=article&utm_campaign=20241128081710\n",
      "\n",
      "27. 현대백화점 문화센터, 겨울학기에 인공지능‧코딩 분야 강좌 개설\n",
      "    링크 : https://www.businesspost.co.kr/BP?command=article_view&num=374607\n",
      "\n",
      "28. \"AI 위험 관리 위해 국제 공조 절실…AI 국가전략 자산화 해야\"\n",
      "    링크 : https://www.newsis.com/view/NISX20241126_0002972373\n",
      "\n",
      "29. 단국대 SW·인공지능 교육 질 높인다…전담 교수 도입\n",
      "    링크 : https://www.newsis.com/view/NISX20241127_0002974457\n",
      "\n",
      "30. 오픈AI-산은, 韓 AI 스타트업 지원·AI 모델 개발 '맞손'\n",
      "    링크 : https://www.newsis.com/view/NISX20241126_0002973067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def get_news_titles(keyword, num_pages):\n",
    "\n",
    "    driver= webdriver.Chrome()\n",
    "\n",
    "    titles = []\n",
    "\n",
    "    for page in range(1,num_pages+1):\n",
    "        url = f\"https://search.naver.com/search.naver?where=news&query={keyword}&start={(page-1)*10+1}\"\n",
    "        driver.get(url)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "\n",
    "\n",
    "        main_titles =soup.select('a.news_tit')\n",
    "\n",
    "        for title in main_titles:\n",
    "            titles.append({\n",
    "                'title':title.get('title'),\n",
    "                'link':title.get('href')\n",
    "            })\n",
    "\n",
    "        print(f\"{page}페이지 완료\")\n",
    "\n",
    "    print(\"\\n=== 검색 결과 ===\")\n",
    "    print(f\"검색어:{keyword}\")\n",
    "    print(f\"수집 시간: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"총 {len(titles)}개의 기사 수집\\n\")\n",
    "\n",
    "    for idx, item in enumerate(titles,1):\n",
    "        print(f\"{idx}. {item['title']}\")\n",
    "        print(f\"    링크 : {item['link']}\")\n",
    "        print()\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "keyword = \"인공지능\"\n",
    "num_pages = 3\n",
    "get_news_titles(keyword,num_pages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "16fefc1a-4d1a-42c0-8822-739396f4cca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색할 키워드를 입력하세요:  인공지능\n",
      "수집할 페이지 수를 입력하세요:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'인공지능' 관련 뉴스 수집을 시작합니다...\n",
      "1페이지 완료\n",
      "2페이지 완료\n",
      "3페이지 완료\n",
      "\n",
      "=== 저장 완료 ===\n",
      "파일 저장 위치: news_data/naver_news_인공지능_20241129_163421.csv\n",
      "총 기사 수: 30개\n",
      "\n",
      "=== 데이터 통계 ===\n",
      "언론사별 기사 수:\n",
      "언론사\n",
      "뉴시스언론사 선정    4\n",
      "뉴시스          3\n",
      "내일신문         3\n",
      "아시아경제        2\n",
      "국민일보         2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== 작업 완료 ===\n",
      "CSV 파일: news_data/naver_news_인공지능_20241129_163421.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class NaverNewsCollector:\n",
    "    def __init__(self):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        \n",
    "    def collect_news(self, keyword, num_pages):\n",
    "        articles = []\n",
    "        \n",
    "        try:\n",
    "            for page in range(1, num_pages + 1):\n",
    "                url = f\"https://search.naver.com/search.naver?where=news&query={keyword}&start={(page-1)*10 + 1}\"\n",
    "                self.driver.get(url)\n",
    "                time.sleep(2)\n",
    "                \n",
    "                soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "                news_items = soup.select(\"div.news_wrap.api_ani_send\")\n",
    "                \n",
    "                for item in news_items:\n",
    "                    try:\n",
    "                        title_element = item.select_one(\"a.news_tit\")\n",
    "                        if not title_element:\n",
    "                            continue\n",
    "                            \n",
    "                        title = title_element['title']\n",
    "                        link = title_element['href']\n",
    "                        \n",
    "                        press = item.select_one(\"a.press\")\n",
    "                        press_name = press.text if press else \"알 수 없음\"\n",
    "                        \n",
    "                        date = item.select_one(\"span.info\")\n",
    "                        date_text = date.text if date else \"날짜 없음\"\n",
    "                        \n",
    "                        articles.append({\n",
    "                            '수집일시': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            '검색어': keyword,\n",
    "                            '제목': title,\n",
    "                            '언론사': press_name,\n",
    "                            '보도일자': date_text,\n",
    "                            '링크': link\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"기사 추출 중 에러: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                print(f\"{page}페이지 완료\")\n",
    "                \n",
    "            return articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"에러 발생: {e}\")\n",
    "            return articles\n",
    "\n",
    "    def save_to_csv(self, articles, keyword):\n",
    "        try:\n",
    "            df = pd.DataFrame(articles)\n",
    "            \n",
    "            output_dir = 'news_data'\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            \n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            filename = f\"{output_dir}/naver_news_{keyword}_{timestamp}.csv\"\n",
    "            \n",
    "            df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "            \n",
    "            print(\"\\n=== 저장 완료 ===\")\n",
    "            print(f\"파일 저장 위치: {filename}\")\n",
    "            print(f\"총 기사 수: {len(articles)}개\")\n",
    "            print(\"\\n=== 데이터 통계 ===\")\n",
    "            print(\"언론사별 기사 수:\")\n",
    "            print(df['언론사'].value_counts().head())\n",
    "            \n",
    "            return filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"CSV 저장 중 에러 발생: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.quit()\n",
    "\n",
    "def main():\n",
    "    keyword = input(\"검색할 키워드를 입력하세요: \")\n",
    "    num_pages = int(input(\"수집할 페이지 수를 입력하세요: \"))\n",
    "    \n",
    "    collector = NaverNewsCollector()\n",
    "    try:\n",
    "        print(f\"'{keyword}' 관련 뉴스 수집을 시작합니다...\")\n",
    "        articles = collector.collect_news(keyword, num_pages)\n",
    "        \n",
    "        if articles:\n",
    "            csv_file = collector.save_to_csv(articles, keyword)\n",
    "            print(\"\\n=== 작업 완료 ===\")\n",
    "            if csv_file:\n",
    "                print(f\"CSV 파일: {csv_file}\")\n",
    "           \n",
    "    finally:\n",
    "        collector.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7bb964-909d-465a-a9a4-fda8e3f5fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "def book():\n",
    "    url = \"https://books.toscrape.com/\"\n",
    "   \n",
    "    options = Options()\n",
    "    #options.add_argument('--headless')  \n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    service = Service('chromedriver.exe')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    try:\n",
    "        driver.get(url)\n",
    "     \n",
    "        time.sleep(5) \n",
    "       \n",
    "        posts = []\n",
    "        items = driver.find_elements(By.CSS_SELECTOR, 'article.product_pod')\n",
    "        \n",
    "        rt = {'One':1,'Two':2,'Three':3,'Four':4,'Five':5}\n",
    "        for item in items:\n",
    "            title = item.find_element(By.CSS_SELECTOR, 'h3 a').get_attribute('title')\n",
    "            price = item.find_element(By.CSS_SELECTOR, 'p.price_color').text.strip()\n",
    "            rate = rt[item.find_element(By.CSS_SELECTOR, 'p.star-rating').get_attribute('class').split()[-1]]\n",
    "    \n",
    "         \n",
    "            posts.append({\n",
    "                'title': title,\n",
    "                'price': price,\n",
    "                'rate':rate\n",
    "            })\n",
    "        return posts\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "book_data = book()\n",
    "for data in book_data:\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7713d3d1-9af6-4362-9585-bb86136494f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Travel 페이지 크롤링 완료.\n",
      "Travel 크롤링 완료.\n",
      "Mystery 페이지 크롤링 완료.\n",
      "Mystery 페이지 크롤링 완료.\n",
      "Mystery 크롤링 완료.\n",
      "Historical Fiction 페이지 크롤링 완료.\n",
      "Historical Fiction 페이지 크롤링 완료.\n",
      "Historical Fiction 크롤링 완료.\n",
      "Sequential Art 페이지 크롤링 완료.\n",
      "Sequential Art 페이지 크롤링 완료.\n",
      "Sequential Art 페이지 크롤링 완료.\n",
      "Sequential Art 페이지 크롤링 완료.\n",
      "Sequential Art 크롤링 완료.\n",
      "Classics 페이지 크롤링 완료.\n",
      "Classics 크롤링 완료.\n",
      "Philosophy 페이지 크롤링 완료.\n",
      "Philosophy 크롤링 완료.\n",
      "Romance 페이지 크롤링 완료.\n",
      "Romance 페이지 크롤링 완료.\n",
      "Romance 크롤링 완료.\n",
      "Womens Fiction 페이지 크롤링 완료.\n",
      "Womens Fiction 크롤링 완료.\n",
      "Fiction 페이지 크롤링 완료.\n",
      "Fiction 페이지 크롤링 완료.\n",
      "Fiction 페이지 크롤링 완료.\n",
      "Fiction 페이지 크롤링 완료.\n",
      "Fiction 크롤링 완료.\n",
      "Childrens 페이지 크롤링 완료.\n",
      "Childrens 페이지 크롤링 완료.\n",
      "Childrens 크롤링 완료.\n",
      "Religion 페이지 크롤링 완료.\n",
      "Religion 크롤링 완료.\n",
      "Nonfiction 페이지 크롤링 완료.\n",
      "Nonfiction 페이지 크롤링 완료.\n",
      "Nonfiction 페이지 크롤링 완료.\n",
      "Nonfiction 페이지 크롤링 완료.\n",
      "Nonfiction 페이지 크롤링 완료.\n",
      "Nonfiction 페이지 크롤링 완료.\n",
      "Nonfiction 크롤링 완료.\n",
      "Music 페이지 크롤링 완료.\n",
      "Music 크롤링 완료.\n",
      "Default 페이지 크롤링 완료.\n",
      "Default 페이지 크롤링 완료.\n",
      "Default 페이지 크롤링 완료.\n",
      "Default 페이지 크롤링 완료.\n",
      "Default 페이지 크롤링 완료.\n",
      "Default 페이지 크롤링 완료.\n",
      "Default 페이지 크롤링 완료.\n",
      "Default 페이지 크롤링 완료.\n",
      "Default 크롤링 완료.\n",
      "Science Fiction 페이지 크롤링 완료.\n",
      "Science Fiction 크롤링 완료.\n",
      "Sports and Games 페이지 크롤링 완료.\n",
      "Sports and Games 크롤링 완료.\n",
      "Add a comment 페이지 크롤링 완료.\n",
      "Add a comment 페이지 크롤링 완료.\n",
      "Add a comment 페이지 크롤링 완료.\n",
      "Add a comment 페이지 크롤링 완료.\n",
      "Add a comment 크롤링 완료.\n",
      "Fantasy 페이지 크롤링 완료.\n",
      "Fantasy 페이지 크롤링 완료.\n",
      "Fantasy 페이지 크롤링 완료.\n",
      "Fantasy 크롤링 완료.\n",
      "New Adult 페이지 크롤링 완료.\n",
      "New Adult 크롤링 완료.\n",
      "Young Adult 페이지 크롤링 완료.\n",
      "Young Adult 페이지 크롤링 완료.\n",
      "Young Adult 페이지 크롤링 완료.\n",
      "Young Adult 크롤링 완료.\n",
      "Science 페이지 크롤링 완료.\n",
      "Science 크롤링 완료.\n",
      "Poetry 페이지 크롤링 완료.\n",
      "Poetry 크롤링 완료.\n",
      "Paranormal 페이지 크롤링 완료.\n",
      "Paranormal 크롤링 완료.\n",
      "Art 페이지 크롤링 완료.\n",
      "Art 크롤링 완료.\n",
      "Psychology 페이지 크롤링 완료.\n",
      "Psychology 크롤링 완료.\n",
      "Autobiography 페이지 크롤링 완료.\n",
      "Autobiography 크롤링 완료.\n",
      "Parenting 페이지 크롤링 완료.\n",
      "Parenting 크롤링 완료.\n",
      "Adult Fiction 페이지 크롤링 완료.\n",
      "Adult Fiction 크롤링 완료.\n",
      "Humor 페이지 크롤링 완료.\n",
      "Humor 크롤링 완료.\n",
      "Horror 페이지 크롤링 완료.\n",
      "Horror 크롤링 완료.\n",
      "History 페이지 크롤링 완료.\n",
      "History 크롤링 완료.\n",
      "Food and Drink 페이지 크롤링 완료.\n",
      "Food and Drink 페이지 크롤링 완료.\n",
      "Food and Drink 크롤링 완료.\n",
      "Christian Fiction 페이지 크롤링 완료.\n",
      "Christian Fiction 크롤링 완료.\n",
      "Business 페이지 크롤링 완료.\n",
      "Business 크롤링 완료.\n",
      "Biography 페이지 크롤링 완료.\n",
      "Biography 크롤링 완료.\n",
      "Thriller 페이지 크롤링 완료.\n",
      "Thriller 크롤링 완료.\n",
      "Contemporary 페이지 크롤링 완료.\n",
      "Contemporary 크롤링 완료.\n",
      "Spirituality 페이지 크롤링 완료.\n",
      "Spirituality 크롤링 완료.\n",
      "Academic 페이지 크롤링 완료.\n",
      "Academic 크롤링 완료.\n",
      "Self Help 페이지 크롤링 완료.\n",
      "Self Help 크롤링 완료.\n",
      "Historical 페이지 크롤링 완료.\n",
      "Historical 크롤링 완료.\n",
      "Christian 페이지 크롤링 완료.\n",
      "Christian 크롤링 완료.\n",
      "Suspense 페이지 크롤링 완료.\n",
      "Suspense 크롤링 완료.\n",
      "Short Stories 페이지 크롤링 완료.\n",
      "Short Stories 크롤링 완료.\n",
      "Novels 페이지 크롤링 완료.\n",
      "Novels 크롤링 완료.\n",
      "Health 페이지 크롤링 완료.\n",
      "Health 크롤링 완료.\n",
      "Politics 페이지 크롤링 완료.\n",
      "Politics 크롤링 완료.\n",
      "Cultural 페이지 크롤링 완료.\n",
      "Cultural 크롤링 완료.\n",
      "Erotica 페이지 크롤링 완료.\n",
      "Erotica 크롤링 완료.\n",
      "Crime 페이지 크롤링 완료.\n",
      "Crime 크롤링 완료.\n",
      "Travel 이미지 저장 완료.\n",
      "Travel 크롤링 완료.\n",
      "Mystery 이미지 저장 완료.\n",
      "Mystery 이미지 저장 완료.\n",
      "Mystery 크롤링 완료.\n",
      "Historical Fiction 이미지 저장 완료.\n",
      "Historical Fiction 이미지 저장 완료.\n",
      "Historical Fiction 크롤링 완료.\n",
      "Sequential Art 이미지 저장 완료.\n",
      "Sequential Art 이미지 저장 완료.\n",
      "Sequential Art 이미지 저장 완료.\n",
      "Sequential Art 이미지 저장 완료.\n",
      "Sequential Art 크롤링 완료.\n",
      "Classics 이미지 저장 완료.\n",
      "Classics 크롤링 완료.\n",
      "Philosophy 이미지 저장 완료.\n",
      "Philosophy 크롤링 완료.\n",
      "Romance 이미지 저장 완료.\n",
      "Romance 이미지 저장 완료.\n",
      "Romance 크롤링 완료.\n",
      "Womens Fiction 이미지 저장 완료.\n",
      "Womens Fiction 크롤링 완료.\n",
      "Fiction 이미지 저장 완료.\n",
      "Fiction 이미지 저장 완료.\n",
      "Fiction 이미지 저장 완료.\n",
      "Fiction 이미지 저장 완료.\n",
      "Fiction 크롤링 완료.\n",
      "Childrens 이미지 저장 완료.\n",
      "Childrens 이미지 저장 완료.\n",
      "Childrens 크롤링 완료.\n",
      "Religion 이미지 저장 완료.\n",
      "Religion 크롤링 완료.\n",
      "Nonfiction 이미지 저장 완료.\n",
      "Nonfiction 이미지 저장 완료.\n",
      "Nonfiction 이미지 저장 완료.\n",
      "Nonfiction 이미지 저장 완료.\n",
      "Nonfiction 이미지 저장 완료.\n",
      "Nonfiction 이미지 저장 완료.\n",
      "Nonfiction 크롤링 완료.\n",
      "Music 이미지 저장 완료.\n",
      "Music 크롤링 완료.\n",
      "Default 이미지 저장 완료.\n",
      "Default 이미지 저장 완료.\n",
      "Default 이미지 저장 완료.\n",
      "Default 이미지 저장 완료.\n",
      "Default 이미지 저장 완료.\n",
      "Default 이미지 저장 완료.\n",
      "Default 이미지 저장 완료.\n",
      "Default 이미지 저장 완료.\n",
      "Default 크롤링 완료.\n",
      "Science Fiction 이미지 저장 완료.\n",
      "Science Fiction 크롤링 완료.\n",
      "Sports and Games 이미지 저장 완료.\n",
      "Sports and Games 크롤링 완료.\n",
      "Add a comment 이미지 저장 완료.\n",
      "Add a comment 이미지 저장 완료.\n",
      "Add a comment 이미지 저장 완료.\n",
      "Add a comment 이미지 저장 완료.\n",
      "Add a comment 크롤링 완료.\n",
      "Fantasy 이미지 저장 완료.\n",
      "Fantasy 이미지 저장 완료.\n",
      "Fantasy 이미지 저장 완료.\n",
      "Fantasy 크롤링 완료.\n",
      "New Adult 이미지 저장 완료.\n",
      "New Adult 크롤링 완료.\n",
      "Young Adult 이미지 저장 완료.\n",
      "Young Adult 이미지 저장 완료.\n",
      "Young Adult 이미지 저장 완료.\n",
      "Young Adult 크롤링 완료.\n",
      "Science 이미지 저장 완료.\n",
      "Science 크롤링 완료.\n",
      "Poetry 이미지 저장 완료.\n",
      "Poetry 크롤링 완료.\n",
      "Paranormal 이미지 저장 완료.\n",
      "Paranormal 크롤링 완료.\n",
      "Art 이미지 저장 완료.\n",
      "Art 크롤링 완료.\n",
      "Psychology 이미지 저장 완료.\n",
      "Psychology 크롤링 완료.\n",
      "Autobiography 이미지 저장 완료.\n",
      "Autobiography 크롤링 완료.\n",
      "Parenting 이미지 저장 완료.\n",
      "Parenting 크롤링 완료.\n",
      "Adult Fiction 이미지 저장 완료.\n",
      "Adult Fiction 크롤링 완료.\n",
      "Humor 이미지 저장 완료.\n",
      "Humor 크롤링 완료.\n",
      "Horror 이미지 저장 완료.\n",
      "Horror 크롤링 완료.\n",
      "History 이미지 저장 완료.\n",
      "History 크롤링 완료.\n",
      "Food and Drink 이미지 저장 완료.\n",
      "Food and Drink 이미지 저장 완료.\n",
      "Food and Drink 크롤링 완료.\n",
      "Christian Fiction 이미지 저장 완료.\n",
      "Christian Fiction 크롤링 완료.\n",
      "Business 이미지 저장 완료.\n",
      "Business 크롤링 완료.\n",
      "Biography 이미지 저장 완료.\n",
      "Biography 크롤링 완료.\n",
      "Thriller 이미지 저장 완료.\n",
      "Thriller 크롤링 완료.\n",
      "Contemporary 이미지 저장 완료.\n",
      "Contemporary 크롤링 완료.\n",
      "Spirituality 이미지 저장 완료.\n",
      "Spirituality 크롤링 완료.\n",
      "Academic 이미지 저장 완료.\n",
      "Academic 크롤링 완료.\n",
      "Self Help 이미지 저장 완료.\n",
      "Self Help 크롤링 완료.\n",
      "Historical 이미지 저장 완료.\n",
      "Historical 크롤링 완료.\n",
      "Christian 이미지 저장 완료.\n",
      "Christian 크롤링 완료.\n",
      "Suspense 이미지 저장 완료.\n",
      "Suspense 크롤링 완료.\n",
      "Short Stories 이미지 저장 완료.\n",
      "Short Stories 크롤링 완료.\n",
      "Novels 이미지 저장 완료.\n",
      "Novels 크롤링 완료.\n",
      "Health 이미지 저장 완료.\n",
      "Health 크롤링 완료.\n",
      "Politics 이미지 저장 완료.\n",
      "Politics 크롤링 완료.\n",
      "Cultural 이미지 저장 완료.\n",
      "Cultural 크롤링 완료.\n",
      "Erotica 이미지 저장 완료.\n",
      "Erotica 크롤링 완료.\n",
      "Crime 이미지 저장 완료.\n",
      "Crime 크롤링 완료.\n",
      "CSV 저장 완료: fake_data/fake_site_Travel_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Mystery_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Historical Fiction_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Sequential Art_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Classics_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Philosophy_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Romance_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Womens Fiction_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Fiction_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Childrens_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Religion_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Nonfiction_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Music_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Default_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Science Fiction_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Sports and Games_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Add a comment_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Fantasy_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_New Adult_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Young Adult_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Science_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Poetry_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Paranormal_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Art_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Psychology_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Autobiography_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Parenting_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Adult Fiction_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Humor_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Horror_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_History_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Food and Drink_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Christian Fiction_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Business_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Biography_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Thriller_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Contemporary_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Spirituality_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Academic_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Self Help_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Historical_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Christian_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Suspense_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Short Stories_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Novels_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Health_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Politics_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Cultural_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Erotica_20241202_105356.csv\n",
      "CSV 저장 완료: fake_data/fake_site_Crime_20241202_105356.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os # dir폴더 만들기 위한 라이브러리\n",
    "import httpx # 이미지저장 라이브러리\n",
    "import ssl # 이미지저장 라이브러리\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "ssl_context = ssl.create_default_context() # 이미지저장 기본값\n",
    "ssl_context.options |= ssl.OP_LEGACY_SERVER_CONNECT # 이미지저장 기본 옵션\n",
    "\n",
    "class WebScraper:\n",
    "    def __init__(self):\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--headless') # 띄운 화면 숨기기\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    def create_driver(self):\n",
    "        return webdriver.Chrome(options=self.options)\n",
    "\n",
    "#------------------------------기본 셋팅----------------------------\n",
    "    # 카테고리 리스트 스크랩\n",
    "    def scrape_category(self):\n",
    "        driver = self.create_driver() # -> driver = webdriver.Chrome(options=options)\n",
    "        try:\n",
    "            driver.get('https://books.toscrape.com')\n",
    "            category = driver.find_elements(By.CSS_SELECTOR, 'ul.nav.nav-list li ul li')\n",
    "            return [{\n",
    "                'category':book.find_element(By.CSS_SELECTOR, 'a').text.strip(),\n",
    "                'link':book.find_element(By.CSS_SELECTOR, 'a').get_attribute('href')\n",
    "            } for book in category]\n",
    "            \n",
    "        finally: # 반드시 실행 , close , quit작업할때 자주사용\n",
    "            driver.quit()\n",
    "\n",
    "    # 책정보 주는 가짜 스크랩 사이트 카테고리별 스크랩 (책 제목, 가격, 평점)\n",
    "    def scrape_fake_book(self, category):\n",
    "        driver = self.create_driver()\n",
    "        rating = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}\n",
    "        books_list=[]\n",
    "        try:\n",
    "            for a in category: \n",
    "                book_list = []\n",
    "                driver.get(a['link'])\n",
    "                while True:\n",
    "                    books = driver.find_elements(By.CSS_SELECTOR, 'article.product_pod')\n",
    "                    for book in books:\n",
    "                        title = book.find_element(By.CSS_SELECTOR, 'h3 a').get_attribute('title')\n",
    "                        price = book.find_element(By.CSS_SELECTOR, '.price_color').text\n",
    "                        rate_class = book.find_element(By.CSS_SELECTOR, 'p.star-rating').get_attribute('class')\n",
    "                        rate = rating[rate_class.strip().split()[-1]]\n",
    "                        book_list.append({'title': title, 'price': price, 'rate': rate})\n",
    "                    \n",
    "                    print(f\"{a['category']} 페이지 크롤링 완료.\")\n",
    "                    \n",
    "                    # 다음 페이지가 있는지 확인\n",
    "                    try:\n",
    "                        next_button = driver.find_element(By.CSS_SELECTOR, 'li.next a')\n",
    "                        next_link = next_button.get_attribute('href')\n",
    "                        driver.get(next_link)\n",
    "                    except Exception:\n",
    "                        books_list.append({a['category']:book_list})\n",
    "                        print(f\"{a['category']} 크롤링 완료.\")\n",
    "                        break\n",
    "            \n",
    "            return books_list\n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    # 이미지 저장\n",
    "    def srape_images(self,category):\n",
    "        driver = self.create_driver()\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            for a in category:\n",
    "                if not os.path.exists(f'images'):\n",
    "                    os.makedirs(f'images')\n",
    "                if not os.path.exists(f'images/{a['category']}'):\n",
    "                    os.makedirs(f'images/{a['category']}')\n",
    "                driver.get(a['link'])\n",
    "                nn=1\n",
    "                while True:\n",
    "                    books = driver.find_elements(By.CSS_SELECTOR, 'img.thumbnail')\n",
    "\n",
    "                    for book in books:\n",
    "                        alt = book.get_attribute('alt')\n",
    "                        img_url = book.get_attribute('src')\n",
    "                        \n",
    "                        if img_url:\n",
    "                            response = httpx.get(img_url, verify=ssl_context)\n",
    "                            #response = requests.get(img_url)\n",
    "                            filename = f'images/{a['category']}/article_{nn}.jpg'\n",
    "                            if response.status_code == 200:\n",
    "                                with open(filename, 'wb') as f:\n",
    "                                    f.write(response.content)\n",
    "                        else:\n",
    "                            filename = None\n",
    "                        nn+=1\n",
    "                    \n",
    "                    print(f\"{a['category']} 이미지 저장 완료.\")\n",
    "                    \n",
    "                    # 다음 페이지가 있는지 확인\n",
    "                    try:\n",
    "                        next_button = driver.find_element(By.CSS_SELECTOR, 'li.next a')\n",
    "                        next_link = next_button.get_attribute('href')\n",
    "                        driver.get(next_link)\n",
    "                    except Exception:\n",
    "                        print(f\"{a['category']} 크롤링 완료.\")\n",
    "                        break            \n",
    "        finally:\n",
    "            driver.quit()\n",
    "\n",
    "    def save_to_csv(self, books):\n",
    "        try:\n",
    "            output_dir = 'fake_data'\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "    \n",
    "            for book_data in books:\n",
    "                for category, data in book_data.items():\n",
    "                    df = pd.DataFrame(data)\n",
    "                    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "                    filename = f\"{output_dir}/fake_site_{category}_{timestamp}.csv\"\n",
    "                    df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "                    print(f\"CSV 저장 완료: {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"CSV 저장 중 에러 발생: {str(e)}\")\n",
    "\n",
    "scp = WebScraper()\n",
    "category = scp.scrape_category()\n",
    "books = scp.scrape_fake_book(category)\n",
    "scp.srape_images(category)\n",
    "csv_filename = scp.save_to_csv(books)\n",
    "for book in books:\n",
    "    print(book)    \n",
    "    print('-'*30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d17ad0-c4f9-4411-9d3d-1533bfe2290e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
